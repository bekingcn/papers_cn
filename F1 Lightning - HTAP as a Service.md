# F1 Lightning: HTAP as a Service
## 摘要
对 HTAP（混合事务和分析处理）系统的持续和日益增长的兴趣记录了数据所有者对在同一数据集上同时运行事务和分析工作负载的强烈兴趣。许多关于 HTAP 的报道工作都是在“新建”系统的背景下出现的，回答了“如果我们可以从头开始为 HTAP 设计一个系统，它会是什么样子？”这个问题。虽然这种方法有很大的优点，并且已经开发了许多有价值的技术，但我们发现自己面临着不同的挑战：在多个事务系统中已经存在大量事务数据，并且需要大量查询通过不“拥有”事务系统的现有联合引擎，支持需要从这种组合中获得透明快速查询和事务的新旧应用程序。本文报告了我们对 F1 Lightning 的设计和体验， 我们为应对这一挑战而构建和部署的系统。我们描述了我们的设计决策、我们实施的一些细节，以及我们在谷歌一些最苛刻的应用程序的生产系统中的经验。

## 1. 介绍
对 HTAP（混合事务和分析处理）系统的强烈研究和商业兴趣表明，数据所有者强烈希望在同一数据集上处理查询和事务。已经有大量与 HTAP 系统相关的研究和开发（其中一些早在“HTAP”一词被创造之前就开始了），并且文献中充满了技术和系统描述。大部分工作假设采用“绿地”方法来解决问题，问题是：理想的 HTAP 系统应该是什么样子，以及需要哪些技术进步才能获得良好的性能。在本文中，我们考虑了一种相关但不同的方法：松散耦合的 HTAP 架构，可以在各种约束下支持 HTAP 工作负载。

简而言之，虽然很好地支持 HTAP 至关重要，但对我们而言，在 Google 的生态系统中启用 HTAP 处理并不是最佳选择。在 Google 中，我们使用多个事务数据存储来服务于大型遗留和新工作负载，并且我们拥有与这些系统松散耦合的联合查询引擎。我们想要一个单一的 HTAP 解决方案，可以跨事务存储的不同选项启用，以避免昂贵的迁移并允许事务存储系统设计的灵活性，并且我们希望通过允许事务系统专注于事务来从关注点分离中受益处理和查询引擎专注于查询处理，重点是分析查询。

因此，我们设计、实施和部署了 Lightning，这是一种松散耦合的 HTAP 解决方案，我们称之为“HTAPas-a-service”。 “HTAP 即服务”是指 Lightning 可以透明地提供应用程序可访问的 HTAP 功能，只需将事务存储中其模式中的一些表标记为“Lightning 表”即可。 （这些是他们希望运行 HTAP 工作负载的表。）在生产中运行和支持 Lightning 的实际物流由 HTAP 服务提供商处理，而不是由访问 Lightning 的应用程序或事务系统提供商处理。

创建读取优化的数据副本的所有工作——保持事务数据的一致和新鲜、管理受控复制以及优化和执行可能跨越事务和闪电表的查询——都由闪电及其集成处理使用联合查询引擎。查询引擎的用户受益于效率和性能的提高，其中许多人甚至不知道 Lightning 的存在。此外，我们注意到这对事务存储也是透明的——我们不需要修改事务存储来提供闪电服务，实际上闪电服务是由不拥有事务存储系统开发的团队开发和维护的。

我们希望我们所面临的挑战和我们做出的决定能引起他人的兴趣并为他人所用。我们在部分中概述了系统 3  后来重点介绍了我们发现有用的技术，包括合并和压缩的向量化列实现（第 4.5);  使用两级模式为 Lightning 如何透明地加速对其数据的查询提供很大的自由度（部分 4.6);  一个我们称为“changepump”的变更数据捕获组件，它可以扩展到新的事务数据存储（部分 4.8);  多数据中心环境中可靠性和效率的方法（第 4.9);  以及与联合查询引擎的紧密集成和协同设计（部分 5). 最后，我们还分享了我们开发的一些工程实践（部分 6)  并证明闪电网络在真实世界条件下成功地实现了其性能目标（部分 7).
 
## 2. 相关工作
许多类型的架构和系统已经在 HTAP 环境中激增。在 HTAP 调查中 [25],  HTAP 解决方案分为“单系统 OLTP 和 OLAP”和“分离 OLTP 和 OLAP 系统”。许多新建系统属于“用于 OLTP 和 OLAP 的单一系统”。其中，使用混合行和列数据组织的系统往往比使用单一数据组织进行摄取和分析的系统具有更好的性能。 F1 Lightning 与单系统方法的不同之处在于，分析系统与 OLTP 系统是分离的，因为我们在 Google 的用户无法轻松地批量迁移到全新的系统。

该调查进一步将使用“分离 OLTP 和 OLAP 系统”的 HTAP 解决方案分为两个子类别：OLTP 和 OLAP 的共享存储或解耦存储。采用共享存储的解决方案通常需要对 OLTP 系统进行修改——事实上，此类系统通常由 OLTP 系统自己开发，以利用现有的分析查询引擎来加速分析查询。 F1 Lightning 以及其他使用解耦存储的公司假设不能对 OLTP 存储进行任何修改。

许多应用程序通过维护一个单独的离线 ETL 过程来使用松散解耦的存储来设置其 HTAP 架构。使用离线 ETL 将数据摄取为列式文件格式往往会受到 OLTP 数据和 OLAP 副本之间的高度滞后的影响。 F1 Lightning 通过与变更数据捕获 (CDC) 机制的集成以及使用内存驻留和磁盘驻留的混合存储层次结构来提高数据新鲜度。由于缺乏标准，该领域的大量定制解决方案也导致重复。 F1 Lightning 通过托管服务在 Google 提供标准解决方案。

接下来，我们将介绍一些与 F1 Lightning 共享相似设计原则和技术的具体系统和实现。

SAP HANA 异步并行表复制 (ATR) [21] 是一种复制架构，可在具有行格式存储的单个现代服务器机器中处理所有 OTLP 工作负载，并使用并行日志重放方案来维护以列格式存储的横向扩展 OLAP 副本。可以根据用户首选的最大可接受陈旧度将查询路由到 OLAP 副本。他们的场景和我们的生产环境存在差异，这些不同的约束导致不同的设计选择。在非常高的层次上，SAP HANA ATR 利用 OLTP 和 OLAP 处理之间更紧密的耦合，而我们的系统有意支持更松散的耦合，从而产生了两个感兴趣的系统。

更详细地说，首先，SAP 架构需要在 OLTP 引擎内部提供一个用于日志传送的接口，这在不修改 OLTP 系统的情况下一般是无法实现的。其次，SAP HANA ATR 中的 OLTP 数据库在单台机器上运行，而 Google 的 OLTP 数据库是地理复制的分布式事务系统。这意味着 Lightning 的变更复制必须处理分布式变更日志并协调并行和分布式日志复制。第三，SAP HANA ATR 使用自己的查询系统进行查询路由，而 Lightning 使用联合查询引擎。最后但同样重要的是，Lightning 的 HTAP 即服务可以轻松且透明地连接（通过联合查询引擎）应用程序到多个 OLTP 系统，允许用户实现良好的 HTAP 性能，而无需将他们的数据迁移出他们首选的 OTLP 系统。

SAP HANA ATR 方法中 OLTP 和 OLAP 处理之间的紧密耦合有助于实现比松耦合设计更低的数据延迟。从一开始选择HTAP的全新存储系统可能会受益于更紧密耦合的 HTAP 系统，例如 SAP HANA ATR。

TiFlash [6] 是 TiDB 的列存扩展 [5]. 它将列存储和矢量化处理层添加到行存储 TiDb。它与 TiDB 查询层紧密集成。 TiFlash 只与 TiDB 保持随意的一致性。这与Lighting不同，Lighting 可以在可查询窗口中提供与源数据库的强快照一致性。
Oracle 数据库内存 [3]  是“用于 OLTP 和 OLAP 的单一系统”的另一个示例。 Oracle DBIM 通过为活动数据维护内存中的列存储来加速 HTAP 工作负载，该列存储在事务上与持久行存储保持一致。使用 Oracle DBIM 无需更改应用程序。

基于中间件的数据库复制系统 [12, 13, 19, 27, 30] 将复制引擎与底层 DBMS 系统解耦，以执行 ETL 以从异构数据库中提取数据。现有的基于中间件的数据库复制工作主要集中在一致性、隔离性和 ETL 过程的性能问题上。这些复制系统没有针对生成用于快速分析的副本进行优化，也没有尝试提供透明的 HTAP 体验。

已经完成了几项工作以使用变更数据捕获 (CDC) 将源数据库中的变更增量提取到单独的存储中以进行分析。与重新导入整个数据集的传统 ETL 方法相比，这些系统通常具有改进的更改传播延迟和更改复制效率。领英Databus [17]  是一个与源无关的分布式变更数据捕获系统，它将来自真实源系统的变更提供给下游应用程序，以构建用于不同目的的专门存储和索引。 Databus 在精神上类似于 Lightning 的 Changepump。与 Lightning 不同，Databus 没有为混合事务和分析处理提供完整的服务解决方案。相反，它是用于构建数据副本的组件。我们认为 Databus 可能是构建完整 HTAP 解决方案的有用组件。

关于与 SQL 查询引擎紧密耦合的 HTAP 解决方案，Wildfire [11]  和 SnappyData [24]  是两个较新的 HTAP 系统，它们使用 Spark 作为计算引擎。 Wildfire 结合 SparkSQL [9]  使用基于 Parquet 构建的列存数据组织的 Wildfire 存储系统 [4]. 它为 OLTP 添加了 Spark API，并为 OLAP 查询扩展了 SparkSQL。查询在 Spark 执行器上运行，操作被推送到 Wildfire 存储服务器。 SnappyData 还使用 Spark 生态系统为事务性、交互式分析和流处理提供统一的接口。它使用 Spark RDD 的混合 [32] 和一个事务存储（Apache Gemfire [2])  作为其存储层。这两个系统具有针对其计算引擎进行了优化的存储系统。与 Lightning 不同，它们仍然是本地 HTAP 系统，需要用户从现有的事务存储中迁移数据。

## 3.	系统总览
我们的 HTAP 解决方案由三个主要部分组成： OLTP 数据库，充当事实来源并公开变更数据捕获或其他变更重放接口； F1 Query [28],  分布式SQL查询引擎；和 Lightning，它维护和提供读取优化的副本。
在 Google，有两个主要的 OLTP 数据库： F1 DB [29] 和Spanner [16].  F1 DB 是作为 Spanner 之上的关系数据库层实现的，它被多个主要的 Google 产品使用，包括 AdWords、Payments 和 Shopping。 Google 的许多应用程序也直接使用 Spanner。

图 1：数据如何通过 Lightning 流经 HTAP 系统的概览。有关 Changepump 和 Lightning 服务器架构的详细信息，请参阅章节 4,  有关 Lightning 与 F1 Query 集成的详细信息，请参阅章节 5.

F1 Query 是一个联合查询引擎，它执行以 SQL 方言编写的查询，内部称为 GoogleSQL（开源为ZetaSQL [7]). 与 OLTP 系统原生的查询层（例如 Spanner SQL [10]),  F1 Query 是一个联合引擎，支持许多不同的内部数据源，包括 F1 DB、Spanner、Mesa [20],  列IO [23],  和BigTable [14].  用户能够编写跨这些系统无缝连接的查询，F1 Query 每天执行超过 1000 亿次用户和应用程序行为查询。

在优化分析查询和优化事务处理之间存在着天然的对立。例如，Ads F1 DB 将表存储在分层架构中，实体在物理存储中交错，以避免分布式事务 [29]. 这种分层模式对分区施加了一些限制。数据库为根表保留适度数量的分区以保持低写入延迟，但大型子表将受益于更多分区以进行高吞吐量读取。另一方面，小表可能有太多分区用于低延迟读取，因为它们从根表继承了相同的分区。通常，F1 DB 和 Spanner 通过使用高效的面向行的存储和索引来优化写入和点查找查询等 OLTP 工作负载，并且用户设计他们的模式以最大化写入吞吐量。不幸的是，这意味着，虽然 F1 Query 能够对这些数据集运行分析查询，但直接在这些数据上的分析查询性能通常不是最理想的。 F1 Query 通过与许多工作人员一起运行分布式分析查询来抵消这一点，但这会产生大量的计算资源成本以提供合理的延迟（见图 2  和图 1  为成本和延迟比较）。

针对这些问题，一些团队设置了将 F1 DB 表复制到 ColumnIO 文件或其他格式的管道，以便进一步分析。这种方法有几个缺点。首先，它会导致工程投资分散以及存储重复（实际上，在某些情况下，多个团队各自保留自己的相同数据的副本，因此复制因子可能远大于 2）。其次，由于 ColumnIO 文件不支持就地更新，因此必须定期将副本作为一个整体重述，这是非常低效的，并且以这种方式维护的数据集也往往具有较差的数据新鲜度，因为只有在下一次重述后才能看到新的更改.第三，用户需要显式更改查询以从副本中读取，并且由于架构互操作性问题或 F1 DB 架构和 ColumnIO 架构之间的其他语义差异，他们可能需要修改查询。最后， 访问权限需要在原始数据库和提取的数据之间保持同步，增加了维护开销和安全漏洞风险。

为了解决这些问题，我们开发了 Lightning，这是一个 HTAP 系统，可将 OLTP 数据库中的数据复制为针对分析查询优化的格式。数据库所有者能够逐个表启用 Lightning 或为整个数据库启用 Lightning。对于每个启用的表，Lightning 的一个组件 Changepump 使用 F1 DB 公开的更改数据捕获机制（或者，对于 Spanner，内部日志传送接口）来检测新的更改。然后 Changepump 将这些更改转发到由各个 Lightning 服务器管理的分区，每个分区都维护Log-Structured Merge (LSM) 树 [26]  由分布式文件系统支持（见图  概览）。当 Lightning 摄取这些更改时，它将更改数据从 OLTP 数据库使用的面向行的写入优化格式转换为针对分析优化的面向列的格式。最后，Lightning 还支持对二级索引和汇总等附加结构的异步维护。这在不影响事务吞吐量的情况下进一步提高了查询性能。

Lightning 以与原始 OLTP 数据库一致的快照方式提供摄取的数据。 F1 DB 和 Spanner 都支持使用时间戳的多版本并发控制，并且提交给 Lightning 的每个更改都保留其原始提交时间戳。 Lightning 保证特定时间戳的读取将产生与在同一时间戳对 OLTP 数据库的读取相同的结果，并且它摄取的所有更改都将以与源数据库等效的完全保真度和语义表示。 F1 Query 利用这一点通过自动重写符合条件的查询以使用 Lightning 来透明地加速查询性能；也就是说，当查询请求在特定时间戳读取 F1 DB 和 Spanner 时，如果该数据在 Lightning 中可用，则查询将改为从 Lightning 读取并受益于改进的性能，而无需最终用户进行任何明确的操作。这种重写是逐表进行的， 这意味着单个查询可以从 Lightning 中读取一些表，并直接从 OLTP 数据库中读取其他表。

将 Lightning 添加到我们的查询生态系统可以实现以下目标：

* 提高分析查询的资源效率和延迟：Lightning 存储针对只读分析查询而不是事务处理优化的数据。

* 简单的配置和重复数据删除：Lightning 不再是工程师使用定制的 ETL 管道开发临时解决方案，而是标准化了流程，并允许数据库所有者通过简单的配置更改来启用 HTAP。

* 透明的用户体验：用户无需更改其 SQL 文本甚至不知道 HTAP 副本即可获得更快的分析查询。

* 数据一致性和数据新鲜度：分析查询在与 OLTP 数据库中的最新版本一致的快照上运行。更改会以低延迟自动从 OLTP 数据库复制。

* 数据安全：F1 Query 在重写查询以使用 Lightning 之前，根据原始 OLTP 数据库的访问权限对查询进行授权。这确保了只有被授权读取原始数据的用户才能使用读取优化的副本。 OTLP 数据库中常用的访问控制功能（如逻辑视图）与 Lightning 的工作方式相同。

* 关注点分离：Lightning 是一个独立的系统，不由 F1 DB 或 Spanner 团队维护。这允许管理这些系统的团队专注于支持有效的事务更新，而闪电团队可以专注于优化分析查询性能。

* 可扩展性：Lightning 可以轻松扩展以支持新的事务数据库。 Lightning 的原始实现仅支持 F1 DB。随后的开发工作将其扩展到支持 Spanner。尽管 F1 DB 和 Spanner 具有相同的底层存储，但它们具有不同的模式、不同的客户端 API 和不同的变更数据捕获机制。原则上，Lightning 可以扩展为在任何提供变更数据捕获机制的 OLTP 数据库上运行。

在以下部分中，我们将更详细地描述 Lightning 的架构以及它如何与 F1 Query 交互。

## 4. Lightning 架构
Lighting 由以下组件组成：

数据存储：数据存储层负责不断地将更改应用到 Lightning 副本。它创建存储在分布式文件系统中的读取优化文件，提供允许查询引擎读取与 OLTP 数据库语义相同的存储数据的 API，并处理数据压缩等后台维护操作。

变更复制：变更复制系统负责跟踪 OLTP 数据库提供的事务日志，并对变更进行分区以分发到相关的数据存储服务器。更改复制系统负责跟踪已应用哪些更改，根据需要重放历史更改，并在添加新表时触发回填。

元数据数据库：数据存储和更改复制组件的状态存储在元数据数据库中。

Lighting Master：Lighting Master 协调其他服务器的操作并维护闪电范围内的状态。

在以下部分中，我们将描述 Lightning 提供的读取语义，然后详细介绍这些组件的设计方式。出于空间的考虑，我们主要关注数据存储和更改复制组件。
 
### 4.1	读语义
Lightning 通过快照隔离支持多版本并发控制。针对 Lightning 驻留表的所有查询都指定了读取时间戳，并且 Lightning 返回与该时间戳的 OLTP 数据库一致的数据。这主要是由谷歌现有的使用该模型的 OLTP 数据库以及应用程序开发人员的期望所推动的，假设该模型成立，他们已经建立了一个庞大的产品生态系统。

由于 Lightning 异步应用 OLTP 数据库的更改日志，因此在 OLTP 数据库中所做的更改对通过 Lightning 的查询可见之前存在延迟。此外，Lightning 支持单个查询可以读取过去多远的上限。这个上限可能是无限的（即 Lightning 可以存储所有更改），但实际上大多数查询都集中在最近的数据上，限制 Lightning 中的数据量可以节省成本。

我们将可以通过Lightning查询的时间戳称为安全时间戳。最大安全时间戳表示 Lightning 已全部摄取到该时间戳为止的所有更改，最小安全时间戳表示可以查询的最旧版本的时间戳。本质上，Lightning 有一个数据库的单一版本快照，截至最小安全时间戳，以及从该点到最大安全时间戳的多版本记录。我们称Lighting维护可查询窗口的安全时间戳范围。在 Google 的生产环境中，可查询窗口通常为十小时。

### 4.2	表和增量
Lightning 将数据组织到 Lightning 表中。数据库表、索引和视图在 Lightning 中都被视为物理表。每个 Lightning 表都使用范围分区划分为一组分区。每个分区都存储在多组件日志结构合并 (LSM) 树中。我们将 LSM 树中的每个组件称为一个增量。

Delta 包含对应 Lightning 表的部分行版本。每个部分行版本由相应行的主键和该版本在 OLTP 数据库中提交时间的时间戳标识。 Lightning 存储三种类型的版本，对应于对源数据所做的突变：

* Inserts：插入包含所有列的值。每行的第一个版本是一个插入。
* Updates：更新包含至少一个非键列的值，并且它们省略了未修改的列的值。
* Deletes：删除仅包含键列的值。删除用作墓碑，以指示应在特定时间戳之后从读取中删除行。

为了简化 delta 维护，Lightning 特意放松了 delta 的内容。单个增量可能包含同一键的多个版本，并且同一分区的不同增量中可能存在重复版本。在增量中，部分行版本由键、时间戳对唯一标识，并且为了支持对特定时间戳的快速查找，增量按键 ASC、时间戳 DESC 排序。

新增一个表时，Lightning 运行一个离线进程来生成表的分区，并为每个从 OTLP 源读取和转换的分区创建一个初始增量。
 
### 4.3	内存驻留增量
当 Lightning 接收更改时，生成的部分行版本首先被写入内存驻留的逐行 B 树。类似于 C-Store 使用的写优化存储 [31]  或 C0 LSM 树 [26],  这允许高更新率，牺牲了读取效率。
一个驻留内存的 delta 最多有两个活跃的写入者，并且有很多读取者。对于每个分区，Lightning 使用一个线程来应用 OLTP 事务日志中的新更改。此外，后台线程会定期运行垃圾收集过程，以删除不再出现在 Lightning 可查询窗口中的版本。 Lightning 使用 B 树结构中的写时复制来处理这些并发需求。
一旦数据被写入内存驻留的 delta，它就可以立即用于查询，这取决于 Changepump 提供的一致性协议。但是，对内存驻留增量的写入并不持久，并且 Lightning 不维护自己的预写日志。在系统故障的情况下，存储在内存中的更改可能会丢失。 Lightning 通过重播 OLTP 数据库的日志从该状态中恢复。
从事务日志中重放数小时的更改通常是不切实际的。为了减少必须重放的数据量，Lightning 会定期将内存驻留的增量检查点到磁盘。因为我们希望检查点操作既快速又便宜，Lightning 将驻留在内存中的 B 树的内容原样写入磁盘而不进行转换。检查点不能直接查询，必须加载到内存中才能读取。
当增量变得太大时，无论是由于每个增量大小限制还是服务器范围的内存压力，Lightning 都会将它们写入磁盘。与检查点不同，此操作包括转置，Lightning 将行内存驻留数据转换为读取优化的列格式，类似于 C-Store 架构中的元组移动器。现有读取将继续从内存驻留增量直到写入完成，此时它们将透明地切换到从磁盘读取。
 

### 4.4	磁盘驻留增量
包含 Lighting 大部分数据的磁盘驻留增量存储在读取优化的列文件中。我们构建了一个具有通用接口的抽象层，允许 Lightning 使用许多不同的文件格式来存储增量。这使我们能够在新格式可用时对其进行试验，并确保我们无需大量工程工作即可过渡到更好、更高效的文件格式。 Lightning 支持多种内部列文件格式，但这里我们将仅介绍目前在生产中使用的一种列文件格式。

每个增量文件存储两部分：数据部分和索引部分。数据部分将行版本存储在 PAX（跨分区属性）中 [8]  布局首先将行划分为行包，然后按列存储在行包中。索引部分包含主键上的稀疏 B 树索引，其中叶条目跟踪每个行包的键范围。索引比数据小得多，通常缓存在Lightning服务器中。
这种布局在范围扫描性能和点查找性能之间取得了很好的平衡。由于我们使用 Lightning 来处理混合工作负载，因此优化所有流量模式的性能非常重要。

### 4.5	Delta 合并
Lightning 在查询请求的特定时间戳读取分区。但是，由于 Lightning 存储的部分行版本可能分散在多个增量中（内存或磁盘驻留），每次读取都必须合并增量并组合行版本以形成完整的行。

Delta 合并包含两个逻辑操作：合并和折叠。合并会删除源增量中的重复更改，并将不同的版本复制到新的增量中。因为源增量和新增量不一定使用公共schema，所以合并在复制行时会执行强制schema。折叠将同一key的多个版本组合成一个版本。
此过程使用结合了合并和聚合运算符的标准 LSM 逻辑的矢量化版本。作为预处理步骤，Lightning 首先枚举需要参与合并的增量；如果主键上有谓词，则可以省略不匹配这些谓词的版本的增量。一旦确定了必须参与合并的增量，Lightning 将在两个重复应用的阶段执行 k 路合并：合并计划生成和合并计划应用。

在合并计划生成中，Lightning 从 k 个输入中的每一个中读取一个键块，并确定要折叠的版本以及我们称为合并计划的结构中的顺序。第一步是确定在这一轮中可以折叠的键的范围。由于单个主节点的多个行版本可以包含在单个增量中，因此 Lightning 必须注意它只会折叠没有漏洞的完整历史记录。

为了更具体地说明这一点，假设 Lightning 正在合并两个输入，D1 和 D2，并且它从每个输入中读取了一个块（我们使用 Ki 表示任意键，其中 Ki < Kj for all i < j，缩写“时间戳”为ts 和“操作”作为 op）：

Lightning 只能折叠 key 和 timestamp 小于 K2，ts : 125 的版本，并且只有 key 小于 K2 的折叠行才能包含在本轮输出中。这是因为 D1 可能有 K2 的附加版本，其时间戳在 100 到 125 之间，但这要等到从 D1 读取下一批时才能确定。因此，在单轮中折叠的版本范围上界是所考虑的所有块中最大key中的最小值。一旦确定了上限，Lightning 就会比较关键值以生成单个排序流。每个唯一键在输出缓冲区中分配一个槽。同一密钥的多个版本被标识为折叠组并分配相同的插槽。最后，边界键的版本在托管缓冲区中分配了一个槽，它们将被折叠并保留到下一轮。

生成合并计划后，Lightning 逐列应用它，将行值复制并聚合到适当的缓冲区中。然后，Lightning 刷新输出缓冲区，并在下一轮使用托管缓冲区作为附加输入。也就是说，双向合并将有第三个输入，其中包含除第一个以外的所有轮次的单行版本，但逻辑不变。

最后，这个过程重复额外的轮次，直到所有输入都用完。

### 4.6	Schema 管理
由于 Lightning 正在复制 OLTP 数据库并透明地提供查询服务，因此它必须处理具有相同语义的 schema 演变。 Lightning 监控源数据库schema的更改（在后面的章节中描述 4.8.3)  并以最少的数据移动和处理自动应用更改。

为此，Lightning 使用了两级schema抽象。第一级是逻辑schema，它从 OLTP schema映射到 Lightning 表schema。逻辑schema包含复杂类型，例如协议缓冲区和 GoogleSQL 结构，以及逻辑映射到更简单类型的类型，例如映射到整数的日期和枚举值。对于特定的逻辑架构，Lightning 随后会生成一个或多个物理schema。物理模式仅包含原始类型，例如整数、浮点数和字符串。 Lightning 的文件格式接口仅在物理schema级别运行。这意味着文件格式实现者不需要了解复杂类型的语义，从而降低了实现新文件格式的工程成本。

图 2：以“Id”作为主键列的部分行版本如何存储在 Lightning 的两级模式设计中的示例。在这种情况下，结构和枚举等类型分别成为物理模式中的字符串和整数，并且除了完整的序列化结构之外，各个结构字段作为列单独存储。 NotSet 值是指示值自上一版本以来未更改的标记。

逻辑模式和物理模式通过逻辑映射连接。映射指定如何将逻辑行转换为物理行，反之亦然。数据在摄取期间从逻辑行转换为物理行，并在读取时作为 LSM 堆栈的一部分再次转换回来。在此设计下如何存储数据的示例如图所示 2.

映射的一个用例是为相同的逻辑数据实现替代存储布局。例如，当存储一个协议缓冲区时，我们有两个选择。 Lightning 可以将其存储为序列化的字节字符串，或者 Lightning 可以将其字段分解为单独的列。前者更适合读取大部分或全部字段的查询，后者更适合只读取少数字段的查询。 Lightning 甚至可以存储这两种布局，通过更多的存储空间以获得两个极端的最佳读取性能，并且它可以在内存中使用与在磁盘上使用不同的布局。

映射还有助于仅元数据的schema更改。 Lightning 可以适应许多常见的schema更改，而无需明确重写磁盘上的数据。例如，如果schema更改将一列添加到表中，则无需修改在该schema更改之前写入的增量； Lightning 可以在读取时简单地生成默认值。同样，如果schema变更，删除了一列，Lightning 实际上无法立即删除该数据，因为它仍然需要在删除操作之前的时间戳处可供查询访问，但该数据不应出现在新schema下执行的查询中。

因此，无论何时发生schema变化，Lightning 都会创建一个新的逻辑schema。schema更改后创建的增量使用新的物理schema本地写入。对于旧的增量，Lightning 会分析原始逻辑schema和新逻辑schema之间的差异，以构建适应schema的逻辑映射。schema适应逻辑映射指定 Lightning 如何从符合旧schema的物理行调整为符合新schema的逻辑行（反之亦然）。在读取时，LSM 堆栈应用schema适应映射以无缝转换为预期schema。

并非所有schema变化都可以使用映射层来处理。在 OLTP 数据库中创建新表时，Lightning 需要生成初始快照。像这样的更改由在不同进程中运行的后台任务工作者执行。

最后，随着时间的推移，一个表可能有许多仅元数据的schema更改，以及许多相关的转换。但是，应用许多转换会给每个查询带来性能开销。为了减少这种开销，Lightning 应用schema转换并将旧数据移动到新schema，作为下一节中描述的压缩过程的一部分。

### 4.7	Delta 压缩
闪电不断摄入变化并产生新的增量。但是，增量的持续累积会增加资源使用率并损害读取性能。为了使这些成本保持可控，Lightning 会定期运行增量压缩操作，以将较小的增量重写为单个较大的增量。

Lightning 运行四种类型的 delta compaction：active compaction、minor compaction、major compaction 和 base compaction。Active compaction 对内存驻留的增量执行增量压缩。Minor compaction和major compaction在多版本磁盘驻留增量上运行压缩。Base compaction在最小可查询时间戳之前的时间戳处生成新的数据快照。在高更新率的情况下，Lightning服务需要频繁地将数据刷新到磁盘并生成新的多版本增量以释放内存。为了保持增量的总数很小，压缩速度必须能够赶上增量刷新速度。我们设计了一个基于大小的增量压缩策略来生成压缩任务。关键思想是 Lightning 在两个不同的压缩任务中运行快速和慢速压缩，即minor compaction和major compaction。 Minor compaction 压缩小的和可能是新的 deltas，而 Major compaction 处理大的和旧的 deltas。当 Lightning 为压缩任务选择要压缩的 delta 时，它使用 delta 大小作为标准。从小尺寸限制开始， Lightning以指数方式增加限制，直到找到两个或更多符合压缩条件的增量。这允许Lightning快速减少小增量的数量，同时避免在连续的压缩任务中重复重写大量数据。

在四个压缩任务中，只有便宜且运行速度快的主动压缩是在Lightning服务中完成的。所有其他三个任务都由Lightning服务规划，但在专门的任务worker上执行，无需与闪电服务器上的读取操作和其他关键工作竞争资源。完成压缩任务后，闪电服务器会异步加载最新的压缩增量以替换旧的增量。

### 4.8	变更复制
我们实现了一个名为 Changepump 的变更跟踪服务。 Changepump 提供跨不同事务源的统一接口，将单个变更数据捕获接口的细节从主要的 Lightning 数据存储层中抽象出来，并提供一种可扩展且有效的方式将事务更改反馈到其客户端，即更改订阅者。 Changepump 为 Lightning 提供了几个好处。

首先，它对系统的其余部分隐藏了各个 OLTP 数据库的详细信息。对于每个支持的事务源，我们在 Changepump 中实现了一个适配器，该适配器将该系统的变更数据捕获格式转换为统一格式。这使得支持新数据源变得相对简单。

其次，它从面向事务的变更日志改编为面向分区的变更日志。单个事务的变更日志记录可能跨越多个不同的 Lightning 分区。每个 Lightning 分区都是独立管理的。为了维护一个分区，Lightning服务只需要一系列必须应用于该分区的更改，而不管每个事务的原始范围如何。 Changepump 服务器产生这些变化的流。最后，Changepump 负责维护事务一致性。它跟踪已应用于闪电服务器的所有更改的时间戳，并发出检查点以推进每个分区的最大安全时间戳。这控制何时数据在闪电服务器中变得可查询。

#### 4.8.1 订阅
单个 Lightning 服务器为许多不同的表管理许多分区。对于每个分区，Lightning 维护对 Changepump 的订阅。订阅指定分区的表和键范围，Changepump 负责将这些更改交付给Lightning服务器。

订阅有一个开始时间戳。 Changepump 只会返回在该时间戳之后提交的更改。时间戳可能是过去的，在这种情况下，Changepump 将根据需要回放变更历史。这允许闪电服务器在系统故障的情况下在适当的时间恢复其订阅。

#### 4.8.2 更改数据
Changepump 订阅返回两种数据：变化更新和检查点时间戳更新。变化更新包含订阅的表键范围中的修改行。每个修改行包括该行的主键、事务中修改的值和操作（插入、更新、删除）。来自同一主键的更改按时间戳升序传递，但跨行传递没有严格的顺序保证。这意味着最大安全时间戳可能因不同的主键而异。

因为维护每个键的时间戳状态是昂贵的，所以 Changepump 实现了检查点机制。 Changepump 定期向每个订阅者发送一个检查点时间戳，表明该时间戳之前的所有更改都已交付。这允许Lightning服务器使用检查点推进其最大安全时间戳，而不是保持每个主键的状态。

出于效率原因，Changepump 不会在每次更改更新时都提供检查点时间戳。 Changepump 从更改日志中批量读取更改，并将工作划分为松散协调的并行子流，这意味着生成检查点的成本不菲。 Changepump 发送检查点时间戳的频率是 Lightning 数据新鲜度和变更处理效率之间的权衡（参见章节 7.1  有关此权衡的详细信息）。

#### 4.8.3	Schema 变更
Lightning 使用两种机制来检测架构更改：延迟检测和急切检测。

Changepump 从 OLTP 数据库接收到的每个更改都带有产生该变更对应的schema版本的注释。对于延迟检测，我们只需检查每个更改以查看它是否引用了 Lightning 以前未见过的schema。如果我们看到这样的更改，我们会暂停对该分区的更改处理，直到我们加载并分析了新的schema。

延迟检测有一个缺点，因为它会增加围绕每个schema变更的更新处理延迟，这会影响通过 Lightning 运行的查询所看到的数据的新鲜度。为了降低这种成本，我们还有一个急切的检测模型，其中后台线程轮询 OLTP 数据库以查看是否发生了任何新的架构更改。

同时使用延迟和急切检测可确保 Lightning 很可能在schema更改发生后立即检测到，而不会在更改处理中出现任何明显的中断（参见图 4  平均数据处理延迟）。只有当架构和数据都被赶上时，闪电安全时间才能提前。

#### 4.8.4 分片
尽管 Changepump 在逻辑上是一个单一的服务，但实际上它是作为一个分片服务实现的，其中许多服务器处理一个单一的变更日志。这意味着单个订阅可以在内部连接到多个 Changepump 服务器。 Changepump 客户端库将多个此类连接合并到单个变更流中，使该过程对 Lightning 服务器实现不可见。

像这样的分片是必要的，因为 Changepump 服务器的分区方式与 Lightning 服务器不同。 Changepump 服务器的分区目标是每个分区看到的新写入数量大致相等，而 Lightning 服务器的分区目标是每个分区中包含的总行数大致相等。

Changepump 还利用更改日志中的物理数据局部性来降低读取多个表的更改的成本，而 Lightning 服务器独立地对每个表进行分区。
 
图 3：二级索引的数据流。维护基表分区的闪电服务器从 Changepump 读取更改，生成相应的索引突变，并将这些突变写入 BigTable。维护索引分区的闪电服务器监视 BigTable 中的变化，并将它们应用到它们的内存驻留增量中。一旦数据在内存中，压缩和读取如图所示 1.
 
#### 4.8.5 缓存
Changepump 服务器还包含最近更改记录的内存驻留缓存。此缓存用于提高变更交付吞吐量并减少对 OLTP 数据库更改日志的昂贵 I/O 操作。此缓存改进了两种情况。首先，Lightning 为每个 Lightning 分区保留多个副本以实现容错。这些副本都订阅了来自 Changepump 的相同更改。缓存允许从中一次性订阅变更日志，然后由这些不同的副本共享。

其次，当闪电服务器重新启动时，它需要将其内存驻留状态赶上最新的时间戳。 Changepump 缓存允许在不读取源更改日志的情况下重放最近的变更，从而减少了服务器恢复所需的时间。

#### 4.8.6 二级索引和视图
除了复制基表，Lightning 还可以维护二级索引和包含单表聚合的物化视图。从 Lightning 的存储引擎来看，索引和视图与标准表没有区别；我们将它们视为派生表。但是，由于原始 OLTP 数据库中不存在派生表，因此 Lightning 使用不同的方法来复制其更改。

派生表的分区不是订阅来自 Changepump 服务器的更改，而是订阅来自生成的“变更日志”的变化。此变更日志实际上是由维护基表分区的Lightning服务器编写的。每当基表更改时，Lightning 服务器将计算并发出每个派生表的相关变化。

精明的读者可能已经注意到，Lightning 需要解决派生表更改日志的shuffling问题。派生表分区的维护者只想订阅特定分区的更改，因此他们希望更改日志按派生表键排序。然而，产生派生表更改的基表分区可能来自多个不同的Lightning服务器维护的日志。

为了解决这个问题，Lightning 使用了 BigTable [14]  作为shuffle和存储介质。维护基表分区的Lightning服务器将派生表更改写入按每个派生表的键顺序排序的 BigTable，维护派生表分区的Lightning服务器将扫描 BigTable 以查找对其本地键范围的更改。这些服务器还根据从 Changepump 接收到的检查点时间戳为派生表写入检查点时间戳。

由于 Lightning 处理已提交事务的变更日志，它永远不会看到回滚，因此派生表更改的写入是幂等的。服务器重启和服务器加载同一个分区将为派生表写入相同的变更日志，实现冗余，并覆盖BigTable中的幂等行。目前，Lightning 仅支持有限的一组查询模式用于物化视图维护，尤其是简单的聚合。这些视图往往是 Google 最常用的类型，因此可以满足我们大多数用户的需求。未来将支持更复杂的物化视图扩展，例如joins。

#### 4.8.7 在线重分区
Lightning 可能需要定期地对现有表进行重新分区，以确保所有分区的负载均匀。为此，Lightning 支持动态更改 Lightning 表上的范围分区，而不会影响数据摄取或查询。范围重新分区方案大多是仅元数据操作，这意味着 Lightning 可以频繁重新分区以适应快速变化的写入流量和快速变化的数据大小。

以下情况Lightning 会拆分分区， 1) 分区的总大小远大于目标分区大小，或 2) 当写入负载接近单个分区的摄取带宽时。
对于第一种情况，Lightning 计算新分区的分割点以重新平衡到目标分区大小。对于第二种情况，Lightning 查看更改日志以查找尚未处理的更改，然后选择将这些未来更改行平均划分的分割点。

一旦有了新分区的分割点，Lightning 就会注册新的非活动分区。这仅仅是一个元数据操作——新分区从共享源分区的增量开始。注册新分区后，Lightning 服务器会为其键范围启动 Changepump 订阅并照常维护它们。

在注册新分区时，Lightning 不会停止维护源分区。由于启动新的 Changepump 订阅存在一些延迟，因此 Lightning 在新分区赶上之前不会使它们处于活动状态。届时，Lightning 将自动停用源分区并激活新分区。源分区将继续用于在停用时正在运行的任何查询，但一旦这些查询完成，Lightning 将删除它们。

由于新分区是通过共享源分区中的数据创建的，因此它们可能包含一些超出新分区边界范围的行。 Lightning 通过在读取时应用过滤器来处理此问题，以确保仅从每个分区返回预期的行。作为涉及新分区的下一个压缩过程的一部分，多余的行最终将被删除。

Lightning 还支持按照类似机制合并分区。当要将 M 个分区合并为 N 个分区时，我们使用源分区的并集生成新分区作为仅元数据操作，其余过程相同。

触发分区合并以将小分区合并为较大的分区，以减少分区过多的开销。如果删除了分区中的许多行并且这些删除超出了可查询窗口，则分区的大小可能会缩小。

### 4.9 容错
Lightning 是 Google 的一项关键线上服务，支持 AdWords 和 Payments 等核心 Google 产品，这些产品的中断会导致直接收入损失。因此，Lightning 旨在应对各个层面的故障。在极端情况下，Lightning 通过可配置的黑名单机制支持表级故障转移回 OLTP 数据库。

#### 4.9.1 处理查询失败
有两种常见的查询时失败。一方面，由于计划内或计划外的服务器维护，无法访问分配给读取分区的服务器。另一方面，由于瞬时网络或 I/O 中断，分配的服务器从存储读取数据时遇到错误。我们使用集群内和跨集群复制来处理这些故障。

在数据中心内，Lightning 将每个分区分配给多个 Lightning 服务器。这些Lightning服务器都订阅了来自 Changepump 的相同更改，并且它们独立地维护它们的内存驻留增量。它们共享同一组磁盘驻留增量和内存驻留增量检查点（磁盘上的数据由分布式文件系统复制）。

只有一个副本会将内存驻留的增量转换为磁盘驻留的列存增量，以避免重复工作。当一个副本完成写入时，它会通知其他副本更新他们的 LSM 树。

这些服务器副本中的任何一个都可以为查询提供服务，因为它们都保持完整状态。查询将在这些副本之间进行负载平衡。Lightning还可以暂时增加热点（更高的查询流量）分区的副本。

Lightning 还会调度服务器更新，以便每个分区最多同时只有一个副本重启，以保持高数据可用性。此外，当服务器重新启动时，它会尽可能尝试从其副本加载更改；这比从 Changepump 读取相同的更改要便宜，甚至考虑到 Changepump 缓存。

完整的 Lightning 服务栈部署在多个数据中心。单个数据中心拥有自己的一组 Changepump 服务器、Lightning 服务器、Task Workers 和 Lightning Master。这些服务器独立于其他数据中心运行，在每个数据中心维护数据的完整副本。在每个数据中心保持独立的 LSM 树会产生更少的协调开销，并且每个数据中心都可以以最小的跨数据中心网络成本维持其状态。

所有数据中心共享同一个元数据数据库。元数据数据库是一种多宿主、高度可靠的数据库服务，因此我们假设它始终可用并且没有单点故障。表schema和表分区在数据中心之间共享。如果更改复制卡在数据中心的一个分区中，则对该分区的读取将故障转移到由不同服务器提供服务的另一个数据中心中的同一分区。

Lightning 复制的事务数据库通常也是多宿主的。但是，Lightning 可以选择在与事务存储完全不同的location进行复制，并且它可以复制到比存储原始数据库更多的location。这意味着这种多宿主功能不仅仅是容错——它也是数据库所有者通过使存储更靠近应用程序运行的location来扩展服务能力并改善查询服务的数据局部性的首选方式。
 
#### 4.9.2 处理摄取失败
当数据已经被摄取到 Lightning 中时，上述容错机制降低了查询失败的风险。但是，可能存在首先无法进行数据摄取的故障。例如，Changepump 服务器可能会崩溃，或者 OLTP 系统的变更数据捕获功能中断可能会让 Lightning 无法识别新的变更。

对于前者，单个 Changepump 服务器的故障通过连接到同一数据中心的不同 Changepump 服务器来处理。任何 Lightning 服务器都可以连接到任何 Changepump 服务器，我们在每个数据中心运行多个 Changepump 副本。

对于后者，未能在 OLTP 系统的CDC中复制一条数据可能会显著增加尚未完成复制的区域的读取延迟。 Changepump 使用负载均衡通道连接到 OLTP 系统的 CDC 组件。当通道检测到不健康区域时，通道会将 Changepump 重定向到从 OLTP 系统的另一个健康区域读取（网络带宽和延迟的成本略有增加）。 Lightning master 还监控每个分区上所有数据中心的 Changepump 延迟——每当一个数据中心分区的延迟大于其他数据中心，并且超出配置的阈值时，Lightning master 将重新启动异常数据中心中的分区，并从健康数据中心复制先前摄取的数据。这使得清除传入数据的阻塞比通过慢速数据中心中的 CDC 赶上单个更改更快。

底层 OLTP 系统的变更数据捕获组件中的全局故障很少见。但是，当这种情况发生时，它可能会在全球范围内阻止Lightning进行数据复制，因为没有健康的数据中心可以快速恢复。为此，Lightning 提供了一种表级故障转移机制。

#### 4.9.3 表级故障转移
Lightning 支持表级黑名单机制。当一个表被列入黑名单时，F1 Query 将自动地使用 OLTP 数据库为该表提供查询服务，替代 Lightning 中的数据。

在某些情况下，表可能会被列入黑名单。例如，Lightning 运行一个离线验证程序，不断检查 Lightning 副本与 OLTP 数据库中存储的数据的一致性。如果检测到数据损坏，Lightning 可以将受影响的表列入黑名单，以防止查询返回不正确的结果。此外，如果 Lightning 中某个表无法处理过高的变更数据摄取时，Lightning 会将该表列入黑名单，以避免降低系统范围的最大安全时间戳。表恢复后，Lightning 会自动将其从黑名单中删除。

如前一节所述，尽管成本要高得多（见表 2)，大多数分析查询仍然可以直接在 OLTP 系统上运行.  因此，虽然不希望在 OLTP 系统上运行所有流量，但这种表级回退机制为我们提供了一种强大的方法来定位和隔离系统其余部分的故障，从而提高整体可用性和可靠性。

出于一些权衡，可能一些用户可能更愿意选择性能更好的延迟数据，而不是使用更新鲜的数据。为了解决这个问题，Lightning 为数据库所有者提供了配置数据在故障转移发生前的延迟程度的选项。还可以添加更复杂的故障转移配置：例如，用户可以配置 Lightning，以便在中断期间需要更新数据的高优先级流量进行故障转移，而低优先级流量留在 Lightning 上读取陈旧数据以避免与高优先级流量竞争超过稀缺的 OLTP 资源。
 
## 5. F1 查询集成
谷歌的技术栈是高度分层的，查询和存储等不同的层由不同的系统管理，并且通常由完全独立的团队开发。虽然这种架构在组织层面具有许多优势，但它可能会导致与 API 边界处不必要的数据序列化和转换相关的成本增加。为了避免这种情况，我们将 F1 Query 标准化为 Lightning 的接口——所有 Lightning 读取都由 F1 Query 提供服务——我们精心设计了两个主要功能：透明查询重写和子计划下推。
### 5.1	透明查询重写
正如我们所提到的，Lightning 对发出查询的用户或应用程序是完全透明的——发出查询的实体继续查询 OLTP 数据库而不修改他们的查询，他们甚至可能不知道 Lightning 的存在。 Lightning 通过与 F1 Query 的快照隔离机制集成来实现这一点。

发送到 F1 Query 的只读查询针对数据库的快照执行。如果需要，用户可以选择特定的时间戳，或者他们可以省略时间戳并让 F1 Query 为他们选择最近的时间戳。 F1 Query 选取的读取时间戳称为查询安全时间戳。大多数查询默认使用查询安全时间戳。

启用 Lightning 时，F1 Query 始终选择 Lightning 的数据库级最大安全时间戳作为查询安全时间戳。这允许所有没有明确时间戳的查询都使用Lightning，但代价是降低了数据的新鲜度。我们积极监控生产实例的健康状况，以确保新鲜度不会降低到无法接受的程度。如果用户手动指定时间戳，则仍使用Lightning，前提是时间戳在Lightning的可查询窗口内。

F1 Query 总是像直接查询 OLTP 数据库一样生成逻辑计划。以这种方式生成逻辑计划可以简化系统的该部分，并确保查询计划行为在语义上与查询 OLTP 数据库相同，包括授权检查等关键功能。然后，如果查询以Lightning可以服务的时间戳运行，F1 Query 将Lightning视为物理规划期间每个表的附加访问路径。在这个阶段，仅Lightning的索引和视图也会暴露给优化器。如果选择 Lightning 作为访问路径，F1 Query 会运行额外的物理规则，例如子计划下推，以针对 Lightning 特性进行优化。

### 5.2 子计划下推
运算符下推是提高查询性能的常用技术。作为一个联合查询引擎，F1 支持多个现有数据源的运算符下推。但是，大多数数据源的下推通常仅限于简单的过滤器，这些过滤器可以在数据源的有限过滤器下推 API 中表达。

F1 Query 最近开发了一个矢量化查询评估引擎来取代其原来的面向行的评估器。通过过渡到面向列的评估器，我们观察到性能提高了一个数量级，我们还借此机会对编码列进行了一些优化（类似于 Procella 中的优化） [15])  并重构代码以使其更加模块化。

由于这项工作，我们能够将矢量化评估器直接嵌入到闪电服务器中。这使 F1 Query 优化器能够考虑将一组丰富的选项下推到 Lightning 服务器中——原则上，任何可以由 F1 Query 评估的操作也可以由 Lightning 评估。

在查询计划时，F1 查询优化器将查询计划拆分为可以在在F1的无状态worker 中执行的单元， 然后推送到闪电服务器。对于下推的部分，序列化的执行计划作为读取请求的一部分发送到Lightning服务器。目前，F1 Query 下推那些不需要做数据shuffling的叶子节点部分，例如过滤器、部分聚合和投影。

Lightning服务器执行的结果在分布式执行期间被序列化并传输给 F1 worker。由于 Lightning 重用了 F1 的评估引擎，因此它使用与 F1 Query 相同的面向列的内存数据格式和wire格式。这种格式类似于 Apache Arrow [1]  并在 F1 生态系统中标准化。使用这种格式，F1 Query 工作人员可以直接反序列化来自 Lightning 服务器的数据，就像他们是 F1 Query worker一样，无需数据转换。

## 6. 工程实践
在本节中，我们描述了在Lightning系统开发过程中遵循的一些工程实践。这些实践不仅被证明对 Lightning 有帮助，而且现在在 Google 的 F1 Query 和其他数据基础设施项目的开发中也得到了更广泛的采用。
### 6.1	可复用的组件
在 Lightning 的开发过程中，我们创建并贡献了许多通用系统组件，这些组件现在也被其他查询、存储和数据处理系统采用。

使 Lightning 组件可重用的一个原因是，为 Lightning 开发的库对系统特定数据结构或 API 的依赖性最小。 Lightning 由构建 F1 Query 的同一团队开发。虽然在 F1 Query 中使用现有数据结构来构建库可能很有吸引力，但这种硬依赖会使 Google 的其他系统更难采用它。相反，我们开发了独立的库，例如 Lightning 的按列内存格式和 F1 Query 的矢量化评估引擎，并考虑了重用。这些库现在在整个 Google 中使用。

该方式另一个优点是它鼓励为更高级别的系统组件定义适当的 API 边界。例如，Changepump 最初是为 Lightning 设计的，但后来成为更通用的变更订阅服务，也被 TableCache 使用 [22],  通过 F1 DB 上的缓存和 Lightning 的内部元数据数据库更改传播进行内存读取。此外，Lightning 的存储层正被用于构建具有替代数据摄取设计的其他存储系统。

### 6.2	正确性验证
由于 Lightning 在 Google 管理关键业务数据库，因此正确性是一项关键要求。我们构建了几个正确性验证框架来帮助在开发过程中发现问题，并且我们继续使用这些验证工具作为我们定期发布过程的一部分来捕获软件错误。

为了验证数据完整性，我们构建了一个数据验证器，它会定期扫描整个数据库，并将 OLTP 数据库中的每一行与 Lightning 中的每一行进行比较。数据验证器限制其并行度以避免OLTP系统过载；在非常大的数据库上检测差异可能需要几个小时到几天的时间。

为了捕捉与查询系统集成的错误，我们使用查询重放工具。重放工具从生产日志中提取所有查询，使用相同的快照时间戳针对事务数据库和闪电网络运行这些查询，并比较规范化的查询结果。我们通过对仅在常量值上不同的所有查询进行重复数据删除，选择一致的查询来规范化查询排序，并过滤掉具有副作用或使用非确定性函数的查询。查询重播工具针对每个不同的规范化查询模式重播一个查询。

查询重放有助于发现不是由持久存储中的数据损坏引起的错误。例如，许多与下推执行相关的错误只能通过查询重放来捕获。查询重放工具还从正在运行的查询中提取延迟和资源指标，这为检测性能回归提供了有用的信号。查询系统中的回归通常会在一周内被捕获。

## 7.	成本和收益
在本节中，我们将讨论Lightning所涉及的运营成本以及它对 Google 实际生产系统的好处。我们使用 Lightning 只执行不属于事务的只读 SQL 查询，并且在 Lightning 上运行非事务性 SQL 查询通常更便宜（见表  进行修改的事务性工作负载仍然直接在 OLTP 系统上运行，因此被排除在成本比较之外。运行这些事务性工作负载的总计算成本只是在启用 Lightning 的数据库中运行分析查询成本的一小部分。

然而，数据复制并不是免费的，尤其是Lightning的资源成本分为两部分：复制层和存储层。

将新表添加到 Lightning 时，创建表的初始快照需要一次性成本。在那之后，复制层增量应用更改，持续成本与 OLTP 数据库中的写入大小大致呈线性关系，假设 OLTP 数据库的更改数据捕获功能支持分区和linear-time change tailing。此成本类似于使用日志传送维护副本的成本，只是它只支付由 Lightning 复制的表的日志传送成本。在存储中，Lightning 以列格式保存复制表的副本。这种开销类似于一些保持面向行和面向列的存储的专用 HTAP 存储系统。除了加速分析查询外，Lightning 还可用于异地复制和将只读工作负载与读/写工作负载隔离开来，用来进一步验证额外的存储是否合理。

对于读取密集型应用程序，高效的分析查询计算节省的大量资源可以使 Lightning 节省网络资源，特别是如果这些 HTAP 副本取代了 OLTP 系统数据的副本。 CPU 和 RAM 等计算资源比 HDD 和 SSD 等存储介质更昂贵的情况仍然普遍存在，因此如果能够显着降低计算成本，则支付存储成本是有意义的。在以下部分中，我们将展示从 Google 生产中收集的一些指标，这些指标展示了Lightning在实际工作负载中的优势。

### 7.1	安全时间戳延迟
图4显示广告实例中的闪电安全时间戳延迟。Lightning观察到的安全时间戳延迟取决于两个明确的架构决策。

首先，Lightning 会尽量减少对提取更改的 OLTP 数据库的影响。由于 Lightning 复制了一个大规模的、异地复制的 OLTP 数据库，因此它有多个用于复制更改的选项。例如，Lightning 可以在将更改提交到单个副本后立即复制更改。但是，这会导致处理写入的副本上出现大量热点，而其他副本上的负载有限。相反，Lightning 仅在将更改提交到所有副本后才会复制更改。这
 
图 4：Lightning最大安全时间戳相对于 Google 生产中的 OLTP 数据库的延迟。

允许单个 Lightning 副本从其本地 OLTP 副本重放更改并更均匀地分配负载，但会增加延迟。同样，Lightning 将更改批量读取到离散时间窗口中，而不是读取每个事务的更改日志，这减少了必须向 OLTP 数据库发出的读取总数。

其次，总的来说，Lightning 更喜欢数据可用性而不是数据新鲜度（受限于第 1 节中讨论的最大延迟） 4.9.3). 例如，Lightning 实例发布的安全时间戳延迟由最慢的表分区的延迟决定，即使存在延迟较低的分区也是如此。同样，F1 Query 以所有数据中心公布的最旧安全时间戳运行 Lightning 查询。使用这些保守的时间戳可确保查询能够从任何副本中读取，从而最大限度地减少在本地系统故障的情况下必须从 OLTP 数据库读取的查询数量。

### 7.2	混合查询工作负载延迟
F1 Query 支持所有工作负载的运行查询：OLTP 查询、OLAP 查询和 ETL 查询。所有这些工作负载都在适用时使用 Lightning，并且 Lightning 的数据存储经过调整以优化混合工作负载的查询性能。表1显示单个 Lightning 实例在各种工作负载类型上的性能特征。

OLTP 查询通常是点查找查询，并在 F1 Query 的中央模式下在单台机器上运行。由于具有稀疏索引、有效缓存和高效 LSM 组件的 PAX 文件布局，Lightning 具有快速的点查找性能。 F1 Query 和 Lightning 上的分布式查询可能在数百个分布式 F1 Query 工作人员和 Lightning 服务器中运行。由于快速范围扫描、二级索引和视图，这些查询非常高效。还有一些系统生成的、常见查询模式的、基于固定一组表的查询也受益于缓存特性。

除了机器生成的查询之外，Lightning 还运行由用户发送的临时分析查询。这些查询往往具有更多样化的查询模式，因此查询延迟也不同；它们通常包含没有适用二级索引的全表扫描，并连接来自其他数据源的数据，例如存储在分布式文件系统中的文件。根据确切的查询模式和读取的数据源，这些查询的运行时间可能长达数分钟。 ETL 查询以 F1 Query 的批处理模式运行，该模式使用 MapReduce 运行 F1 Query 的执行内核 [18]  分布式计算框架。这些是资源密集型查询，倾向于扫描整个表、计算连接、转换数据并将结果写入分布式文件系统。几个昂贵的 ETL 查询可以链接在一起在单个管道中运行。这些查询的运行时间从几分钟到几个小时不等。尽管从数据源扫描只是查询成本的一部分，但由于列文件格式和子计划下推，使用 Lightning 服务这些扫描往往会节省大量资源并减少 ETL 延迟。

表 1：在 Lightning 上运行的各种工作负载的延迟分布。所有工作负载都由同一个 Lightning 实例提供服务。

表 2：相对于写入优化事务存储的查询，闪电网络查询的 CPU 效率改进。 “小”查询使用不到 1 秒的 CPU，“中”查询使用 1 秒到 100 秒，“大”查询使用超过 100 秒。

使用单个 Lightning 实例来服务于不同的工作负载可以避免数据所有者和查询引擎决定为每个查询模式选择哪个存储引擎。

### 7.3	CPU效率比较
表2比较了在 Lightning 和 OLTP 数据库（本例中为 F1 DB）上执行相同查询时的 CPU 效率。

我们使用我们的查询重放工具以相同的快照时间戳在 Lightning 和 F1 DB 上运行每个不同的生产查询。这些回放仅限于非 ETL 查询，因为 ETL 查询都有副作用，例如具体化结果。我们比较了在两个存储系统中运行同一组查询的总 CPU 时间。总 CPU 时间由生产中出现的查询模式的出现次数加权。

我们分别报告了闪电服务器和共享 F1 查询工作人员中所用 CPU 的比较。我们还根据 CPU 成本将查询分组到存储桶中：小型、中型和大型。小查询是那些使用少于 1 CPU 秒的查询。中是 CPU 时间在 1 秒到 100 秒之间的查询。大是 CPU 时间大于 100 秒的查询。

我们可以看到 Lightning 在所有三个查询中的 CPU 效率更高，并且CPU 效率在数据源服务器和F1 Query服务器上都得到了提高。

在小型查询中，CPU 改进并不那么显着，因为 OLTP 数据库已经针对事务处理过程中常见的低延迟点读取进行了优化。尽管如此，Lightning 仍然显示出明显的改进。

CPU 效率的提升在中等查询上最为显着。Lightning服务器的节省来自于列存的文件布局，有助于减少读取结构化协议缓冲区的 I/O 和数据处理，支持扫描顺序数据访问的范围分区，以及存储访问和下推操作中的矢量化数据处理。下推操作可以在 F1 Query 服务器和 Lightning 服务器上节省 CPU。在 F1 Query 服务器中，这些操作不再需要评估，将这些操作下推到 Lightning 可以减少在过滤器和聚合的情况下需要序列化和通过网络传输的数据量。

大型查询的 CPU 节省小于中型查询。这是因为大型查询通常会读取大量字节并将这些字节流式传输到 F1 和客户端。当查询读取整个序列化协议缓冲区时，Lightning 的面向列的存储没有帮助。大型查询的成本通常由数据序列化和网络传输主导；尽管 Lightning 通过在 F1 Query 和 Lightning 服务器之间共享通用数据表示来降低成本，但它仍然发挥着重要作用。

## 8. 未来的工作
当然，未来工作的巨大空间仍然存在。目前，Lightning 支持 Google 的两个 OLTP 系统，都支持变更数据捕获。未来的方向可能是将闪电网络扩展到事务源之外或扩展到没有变更数据捕获组件。对于这些，Lightning 可能需要一种变更复制模式，适应更宽松的一致性保证和可能更高的延迟。

另一个基本问题是，像Lightning这样的系统如何真正与现有系统解耦。 Lightning 与其服务的任何事务系统松散耦合，但与一个查询引擎（F1 Query）紧密耦合。对我们来说，这个决定是合理的，因为 F1 Query 的 GoogleSQL 接口使得在遗留应用程序和它们的 OLTP 存储之间插入 Lightning 变得很简单——如果应用程序使用 GoogleSQL，那么它在很大程度上已经与查询引擎无关。此外，与更改 OLTP 系统不同，更改查询引擎不需要将数据迁移到新系统。尽管如此，有趣的是推测是否有可能或有益于使像 Lightning 本身这样的系统与查询引擎无关以及与 OLTP 引擎无关。

## 9. 结论
HTAP 是数据管理的一个广泛而重要的子领域。我们相信这不是一个简单的单一维度的领域，其中方法或系统通过几个指标按质量的总顺序排列就能解决。相反，它是一个复杂的空间，必须在多个维度上权衡系统和方法。基于特定目标部署的目标，其中一些维度将比其他维度更重要。

我们发现自己处于一个场景中，其中重要属性包括在多个 OLTP 系统上透明地提高 HTAP 性能的能力，而无需修改 OLTP 系统或将用户迁移到新系统，与现有的联合查询引擎无缝协作，以及支持严格的地理复制操作正确性和性能要求。我们将为此场景构建的系统称为 Lightning，并将其功能描述为“HTAP 即服务”。

在本文中，我们展示了 Lightning 如何在现有事务存储系统之上对混合查询工作负载启用高性能分析查询。我们已经在 Google 的生产环境中大规模部署了 Lightning 用于关键业务交易数据库，包括 AdWords 和 Payments，并且在不影响查询语义的情况下，在计算资源和查询延迟方面实现了多达数量级的节省。

## 致谢
我们要感谢 Haris Volos 以及实习生 Wangda Zhang 和 Michael Whittaker 在早期开发过程中所做的工作。感谢我们在 Google 上的众多合作者，尤其是 Spanner、F1 DB 和 Mesa 团队。也非常感谢 Google 的早期 Lightning 采用者的耐心和反馈。感谢 James Balfour、Race Wright 和 Jeff Korn 对本文的反馈。最后，非常感谢我们的 F1 Query 和 F1 SRE 同事的支持和反馈，没有他们，Lightning 运行的生态系统将无法实现。
